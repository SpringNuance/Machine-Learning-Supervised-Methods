{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n",
      "The average F1: 0.0\n",
      "The average maximum margin achieved in the training: 0.0\n"
     ]
    }
   ],
   "source": [
    "## ####################################################\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "## ###################################################\n",
    "\n",
    "# load the data\n",
    "X, y = load_breast_cancer(return_X_y=True)  ## X input, y output\n",
    "## to convert the {0,1} output into {-1,+1}\n",
    "y = 2*y - 1\n",
    "\n",
    "X = normalize(X, norm='l2')\n",
    "\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "mdata,ndim = X.shape\n",
    "\n",
    "\n",
    "## learning parameters\n",
    "nitermax = 50  ## maximum iteration\n",
    "eta = 0.1      ## learning speed\n",
    "\n",
    "nfold = 5         ## number of folds\n",
    "cselection = KFold(n_splits=nfold, random_state=None, shuffle=False)\n",
    "## initialize the learning parameters for all folds\n",
    "f1 = np.zeros(nfold)\n",
    "maxmargin_train = np.zeros(nfold)\n",
    "\n",
    "\"\"\"\n",
    "To do ....\n",
    "\n",
    "\"\"\"\n",
    "pos = 0\n",
    "for train_index, test_index in cselection.split(X):\n",
    "    w = np.zeros(ndim)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    iteration = 0\n",
    "    max_margin = -np.inf\n",
    "\n",
    "    classifier = SGDClassifier(max_iter=nitermax, eta0=eta)\n",
    "    #computing the score\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Apply the trained weight\n",
    "    y_pred = np.zeros(y_test.shape[0])\n",
    "    for i in range(y_test.shape[0]):\n",
    "        y_hat = np.dot(w, X_test[i])\n",
    "        if y_hat <= 0:\n",
    "            y_pred[i] = -1\n",
    "        else:\n",
    "            y_pred[i] = 1\n",
    "\n",
    "    #computing the score\n",
    "    f1_loc = f1_score(y_test, y_pred)\n",
    "    f1[pos] = f1_loc\n",
    "    pos += 1\n",
    "\n",
    "\n",
    "print('The average F1:',np.mean(f1))\n",
    "print('The average maximum margin achieved in the training:',np.mean(maxmargin_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ####################################################\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "## ###################################################\n",
    "\n",
    "class logreg_sgd_cls:\n",
    "    def __init__(self, eta, nitermax = 10):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "        eta: real learning speed\n",
    "        nitermax: number of maximum iteration\n",
    "        \"\"\"\n",
    "\n",
    "        self.eta = eta\n",
    "        self.nitermax = nitermax\n",
    "        self.w = None   ## learning weights\n",
    "        self.margimax = 0 ## maximum marginin the training\n",
    "        return\n",
    "    \n",
    "    ## --------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Task: to solve the logistic regression problem\n",
    "            by applying stochastic gradient algorithm\n",
    "        Input: X  2d array of input examples in the rows \n",
    "               y  1d(vector) array of +1,-1 labels\n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        ## initialize the weights \n",
    "        W = np.zeros(n)\n",
    "        self.marginmax = 0\n",
    "        \n",
    "        ## iteration on the full data \n",
    "        for t in range (self.nitermax): \n",
    "            ## iteration on the examples \n",
    "            for i in range (m):\n",
    "                xymargin = y[i]*np.dot (w,X[i]) ## functional margin\n",
    "                if xymargin > self.marginmax: ## find the maximum of the margin\n",
    "                    self.marginmax = xymargin\n",
    "                ## compute the stochastic gradient\n",
    "                philog = 1/(1+ np.exp(-(-xymargin))) ## to agree with the slide \n",
    "                deltaJ = -philog * y[i] * X[i]\n",
    "                W = W - self.eta * deltaJ ## update the weights\n",
    "            self.w = W\n",
    "\n",
    "            return\n",
    "        \n",
    "        ## -----------------------------------\n",
    "    \n",
    "    def predict(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Task: to predict the labels for the given examples based on the self.w \n",
    "        Input:  X 2d array of input examples in the rows\n",
    "        Output: y 1d array of predicted labels\n",
    "        \"\"\"\n",
    "        xw= np.dot (X, self.w)\n",
    "        ## predicting +1 probbaility\n",
    "        Pyplus = 1/(1+np.exp(-xw))\n",
    "        ## predicting -1 probability\n",
    "        Pyminus = 1/(1+np.exp(xw))\n",
    "\n",
    "        ## labels correspond to the greater probabilities \n",
    "        y = 2*(Pyplus > Pyminus)-1\n",
    "        return y\n",
    "    \n",
    "    ## #######:\n",
    "    def main(iworkmode):\n",
    "        # load the data\n",
    "        X, y = load_breast_cancer (return_X_y=True) ## X input, y output\n",
    "        ## to convert the {0,1} output into {-1,+1}\n",
    "        y = 2*y - 1\n",
    "\n",
    "        print(X.shape, y.shape)\n",
    "        mdata, ndim = X.shape\n",
    "        nitermax = 50 ## maximum iteration\n",
    "        eta = 0.1 ## learning speed\n",
    "        nfold = 5 ## number of folds\n",
    "\n",
    "        ## split the data into 5-folds\n",
    "        cselection=KFold(n_splits=nfold,random_state=None, shuffle=False)\n",
    "        iscale = 2 ## = 0 noscaling,\n",
    "                   ## = 1 scaling by row wise L2 norm\n",
    "                   ## = 2 scaling the rows by maximum absolute value, L infinite norm of columns\n",
    "        if iscale == 1:\n",
    "            xnorm = np.sqrt(np.sum(X**2, 1))\n",
    "            xnorm = xnorm + (xnorm == 0)\n",
    "            X /= np.outer(xnorm, np.ones(ndim))\n",
    "        elif iscale == 2:\n",
    "            X /= np.outer(np.ones(mdata), np.max(np.abs(X), 0))\n",
    "        \n",
    "        ## construct a learning object\n",
    "        clogreg = logreg_sgd_cls(eta, nitermax=nitermax)\n",
    "\n",
    "        ## initialize the learning results for all folds\n",
    "        xf1 = np.zeros(nfold)\n",
    "        xprecision = np.zeros(nfold)\n",
    "        xrecall = np.zeros(nfold)\n",
    "        xmargin = np.zeros(nfold)        \n",
    "\n",
    "        ## run the cross-validation\n",
    "        ifold = 0\n",
    "        for index_train, index_test in cselection.split(X):\n",
    "            Xtrain = X[index_train]\n",
    "            ytrain = y[index_train]\n",
    "            Xtest = X[index_test]\n",
    "            ytest = y[index_test]\n",
    "            mtrain = Xtrain.shape[0]\n",
    "            mtest = Xtest.shape[0]\n",
    "            print(\"Training size:\",mtrain)\n",
    "            print(\"Test size:\", mtest)\n",
    "            clogreg.fit(Xtrain, ytrain) ## training\n",
    "            yprediction = clogreg.predict(Xtest) ## prediction\n",
    "\n",
    "            true_positive = np.sum((ytest>0)* (yprediction>0))\n",
    "            true_negative = np.sum((ytest<=0)*(yprediction<=0))\n",
    "            false_positive = np.sum ((ytest<=0)*(yprediction>0))\n",
    "            false_negative = np.sum ((ytest>0)*(yprediction<=0))\n",
    "            precision = true_positive/(true_positive+false_positive)\n",
    "            recall= true_positive/(true_positive+false_negative)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            xprecision [ifold] = precision\n",
    "            xrecall[ifold] = recall \n",
    "            xf1[ifold] = f1\n",
    "            xmargin[ifold] = clogreg.marginmax\n",
    "\n",
    "            ## print('tp, fp, fn, tn:', true_positive, false_positive, false negative, true_negative) \n",
    "            print('Fold, f1, precision, recall:', ifold, '%5.3f'%f1, \\\n",
    "                    '%5.3f'%precision, '%5.3f '%recall)\n",
    "            print('Maximum margin:', '%7.4f'%xmargin [ifold])\n",
    "            ifold += 1\n",
    "        print ('The average F1:', '%5.4f '%np.mean(xf1))\n",
    "        print('The average maximum margin:', '%7.4f '%np. mean (xmargin))\n",
    "        return\n",
    "# ## ######################\n",
    "# ## ######################\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv)==1:\n",
    "#         iworkmode=0\n",
    "#     elif len(sys.argv)>=2:\n",
    "#         iworkmode=eval(sys.argv[1])\n",
    "# main(iworkmode)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bc1a11519d29b3ed6f08646f3ece60640217e649724c6bcfd38e1173c1a1bce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
